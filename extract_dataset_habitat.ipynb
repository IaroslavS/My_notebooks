{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# from habitat.utils.visualizations import maps\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import ast\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_with_hdf5_datasets = '/media/cds-s/data/Datasets/Habitat/HPointLoc/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation_of_alignment = np.array([[ 0.9046169, 0.06094708, -0.42184561],\n",
    "#                                   [ 0.42388538, -0.02507164, 0.90536876],\n",
    "#                                   [ 0.04460322, -0.99782607, -0.04851481]])\n",
    "# rotation_of_alignment = np.vstack((rotation_of_alignment, np.array([0,0,0])))\n",
    "# Translation_of_alignment = np.array([[1.39771048], [2.91022104], [0.059372], [1]])\n",
    "# transform_matrix = np.hstack((rotation_of_alignment, Translation_of_alignment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/cds-s/data/Datasets/Habitat/temp_datasets_hdf5/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e44dde3c4303>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename_hdf5_dataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_with_hdf5_datasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_with_hdf5_datasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_hdf5_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moutput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdir_with_hdf5_datasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdir_with_hdf5_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/temp_datasets/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mfilename_hdf5_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfilename_hdf5_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/cds-s/data/Datasets/Habitat/temp_datasets_hdf5/'"
     ]
    }
   ],
   "source": [
    "for filename_hdf5_dataset in tqdm(os.listdir(dir_with_hdf5_datasets)):\n",
    "    file = h5py.File(os.path.join(dir_with_hdf5_datasets, filename_hdf5_dataset), 'r')\n",
    "    output_path = dir_with_hdf5_datasets[:dir_with_hdf5_datasets.rfind('/')]+'/temp_datasets/'+ \\\n",
    "                  filename_hdf5_dataset[:filename_hdf5_dataset.rfind('.')]\n",
    "    \n",
    "    rgb = file['rgb']\n",
    "    depth = file['depth']\n",
    "    semantic = file['semantic']\n",
    "    objectgoal = file['objectgoal']\n",
    "    compass = file['compass']\n",
    "    gps = file['gps']\n",
    "    heading = file['heading']\n",
    "    top_down_map = file['top_down_map']\n",
    "    fog_of_war_mask = file['fog_of_war_mask']\n",
    "    agent_map_coord = file['agent_map_coord']\n",
    "    agent_angle = file['agent_angle']\n",
    "    mapping = np.array(file['mapping'])\n",
    "    index_to_title_map = ast.literal_eval(str(np.array(file['index_to_title_map']))[2:-1])\n",
    "    positions = file['position']\n",
    "    rotations = file['rotation']\n",
    "    continue\n",
    "    \n",
    "    os.makedirs(os.path.join(output_path,'rgb'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_path,'depth'), exist_ok=True)\n",
    "    depth_file = open(os.path.join(output_path, 'depth.txt'), 'w')\n",
    "    rgb_file = open(os.path.join(output_path, 'rgb.txt'), 'w')\n",
    "    gt_filename = os.path.join('/media/cds-s/data/Datasets/Habitat/temp_gt/',filename_hdf5_dataset.split('/')[-1])\n",
    "    gt_filename = gt_filename[:gt_filename.rfind('.')] + '.txt'\n",
    "    gt_file = open(gt_filename, 'w')\n",
    "    rgb_file.close()\n",
    "    depth_file.close()\n",
    "    gt_file.close()\n",
    "    depth_file = open(os.path.join(output_path, 'depth.txt'), 'a')\n",
    "    rgb_file = open(os.path.join(output_path, 'rgb.txt'), 'a')\n",
    "    gt_file = open(gt_filename, 'a')\n",
    "    for i in range(3):\n",
    "        depth_file.write('ignore\\n')\n",
    "        rgb_file.write('ignore\\n')\n",
    "    for i in range(len(rgb)):\n",
    "        cv2.imwrite(os.path.join(output_path, 'rgb', str(i).zfill(6)+'.png'), rgb[i])\n",
    "#         depth_image = depth[i]*65536.0/10000.0\n",
    "        \n",
    "#         depth_image = depth[i]\n",
    "#         depth_image[depth_image > 0] = depth_image[depth_image >0]*14.5+0.5\n",
    "        \n",
    "        cv2.imwrite(os.path.join(output_path, 'depth', str(i).zfill(6)+'.exr'), depth_image)\n",
    "        rgb_file.write('{:.3f} '.format(i/10))\n",
    "        rgb_file.write('rgb/'+str(i).zfill(6)+'.png\\n')\n",
    "        depth_file.write('{:.3f} '.format(i/10))\n",
    "        depth_file.write('depth/'+str(i).zfill(6)+'.exr\\n')\n",
    "        \n",
    "        r = R.from_quat([rotations[i, 1], rotations[i, 2], rotations[i, 3], rotations[i, 0]])\n",
    "        matrix = r.as_matrix()\n",
    "        pose = np.eye(4)\n",
    "        pose[:3,:3] = matrix\n",
    "        pose[:3, 3] = [positions[i, 2], positions[i, 0], positions[i, 1]]\n",
    "        #pose = np.dot(transform_matrix, pose)\n",
    "        quat_pose = R.from_matrix(pose[:3, :3]).as_quat()\n",
    "        \n",
    "        gt_file.write('{:.3f} {:.8e} {:.8e} {:.8e} {:.8e} {:.8e} {:.8e} {:.8e}\\n'\\\n",
    "                      .format(i/10, -positions[i, 0], positions[i, 1], -positions[i, 2],\n",
    "                        rotations[i, 0], rotations[i, 1], rotations[i, 2], rotations[i, 3]))\n",
    "#         gt_file.write('{:.3f} {:.8e} {:.8e} {:.8e} {:.8e} {:.8e} {:.8e} {:.8e}\\n'\\\n",
    "#                       .format(i/10, pose[0, 3], pose[1, 3], pose[2, 3], \\\n",
    "#                               quat_pose[0], quat_pose[1], quat_pose[2], quat_pose[3]))\n",
    "    \n",
    "    rgb_file.close()\n",
    "    depth_file.close()\n",
    "    gt_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5536"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(depth_image)/np.median(depth[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \" /media/cds-s/data/Datasets/KITTI_Odometry/dataset/sequences/09/000001.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/media/cds-s/data/Datasets/KITTI_Odometry/dataset/sequences/09/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
